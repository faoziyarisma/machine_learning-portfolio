{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Sarcasm Detection with LSTM Algorithms**\n",
        "NLP project about detecting sarcasm with Bi-LSTM TensorFlow. This project involved binary classification, where one 1 represents sarcasm and one 0 does not. Furthermore, this model used dropout to prevent overfitting and used binary crossentropy as the loss function, Adam as the optimizer, and accuracy as the metric. This model had a train accuracy of 87% and a validation accuracy of 82%."
      ],
      "metadata": {
        "id": "8ztWHbUJLSr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ],
      "metadata": {
        "id": "bx7qkIieKusm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4Bl02ECKMhe"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build and train a classifier for the sarcasm dataset"
      ],
      "metadata": {
        "id": "fpyzhuiOKx6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    # load data from json file\n",
        "    with open('sarcasm.json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # prepare data to sentences and labels\n",
        "    for i in data:\n",
        "        sentences.append(i['headline'])\n",
        "        labels.append(i['is_sarcastic'])\n",
        "\n",
        "    # split data\n",
        "    train_sentences = sentences[0:training_size]\n",
        "    train_labels = labels[0:training_size]\n",
        "    val_sentences = sentences[training_size:]\n",
        "    val_labels = labels[training_size:]\n",
        "\n",
        "    # Fit your tokenizer with training data\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "\n",
        "    # Generate index to dictionary of vocabularyn\n",
        "    word_index = tokenizer.fit_on_texts(train_sentences)\n",
        "    word_index = tokenizer.word_index\n",
        "\n",
        "    # train data\n",
        "    sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "    padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, padding=padding_type,truncating=trunc_type)\n",
        "\n",
        "    # validation data\n",
        "    val_sequences = tokenizer.texts_to_sequences(val_sentences)\n",
        "    val_padded = tf.keras.preprocessing.sequence.pad_sequences(val_sequences, maxlen=max_length, padding=padding_type,truncating=trunc_type)\n",
        "\n",
        "    # Convert labels to arrays\n",
        "    train_labels_final = np.array(train_labels)\n",
        "    val_labels_final = np.array(val_labels)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        # tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(24)),\n",
        "        tf.keras.layers.Dropout(0.7),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(padded, train_labels_final, validation_data=(val_padded, val_labels_final), epochs=10, verbose=2)\n",
        "    return model"
      ],
      "metadata": {
        "id": "X7SaMe3nKkF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting and Save Model Classifier"
      ],
      "metadata": {
        "id": "oN_vRN97LIvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ],
      "metadata": {
        "id": "XNcQqioDKpZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb6de92-711c-4687-abd4-0e56d913efbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 - 27s - loss: 0.4623 - accuracy: 0.7688 - val_loss: 0.3939 - val_accuracy: 0.8216 - 27s/epoch - 43ms/step\n",
            "Epoch 2/10\n",
            "625/625 - 8s - loss: 0.3760 - accuracy: 0.8305 - val_loss: 0.3872 - val_accuracy: 0.8207 - 8s/epoch - 13ms/step\n",
            "Epoch 3/10\n",
            "625/625 - 9s - loss: 0.3487 - accuracy: 0.8469 - val_loss: 0.3842 - val_accuracy: 0.8190 - 9s/epoch - 14ms/step\n",
            "Epoch 4/10\n",
            "625/625 - 7s - loss: 0.3346 - accuracy: 0.8551 - val_loss: 0.3883 - val_accuracy: 0.8196 - 7s/epoch - 11ms/step\n",
            "Epoch 5/10\n",
            "625/625 - 8s - loss: 0.3242 - accuracy: 0.8598 - val_loss: 0.3760 - val_accuracy: 0.8284 - 8s/epoch - 12ms/step\n",
            "Epoch 6/10\n",
            "625/625 - 7s - loss: 0.3152 - accuracy: 0.8651 - val_loss: 0.3818 - val_accuracy: 0.8283 - 7s/epoch - 11ms/step\n",
            "Epoch 7/10\n",
            "625/625 - 9s - loss: 0.3085 - accuracy: 0.8688 - val_loss: 0.3818 - val_accuracy: 0.8293 - 9s/epoch - 14ms/step\n",
            "Epoch 8/10\n",
            "625/625 - 6s - loss: 0.3027 - accuracy: 0.8732 - val_loss: 0.4001 - val_accuracy: 0.8167 - 6s/epoch - 10ms/step\n",
            "Epoch 9/10\n",
            "625/625 - 8s - loss: 0.2956 - accuracy: 0.8763 - val_loss: 0.4081 - val_accuracy: 0.8210 - 8s/epoch - 12ms/step\n",
            "Epoch 10/10\n",
            "625/625 - 7s - loss: 0.2934 - accuracy: 0.8787 - val_loss: 0.3935 - val_accuracy: 0.8247 - 7s/epoch - 11ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GwOrMm3dPVB8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}